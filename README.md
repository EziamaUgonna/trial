# [CS:228 - Probabilistic Graphical Models](https://cs.stanford.edu/~ermon/cs228/index.html)

<img src="https://github.com/SKKSaikia/CS228_PGM/blob/master/cs228.PNG">

Probabilistic graphical models are a powerful framework for representing complex domains using probability distributions, with numerous applications in machine learning, computer vision, natural language processing and computational biology. Graphical models bring together graph theory and probability theory, and provide a flexible framework for modeling large collections of random variables with complex interactions. This course will provide a comprehensive survey of the topic, introducing the key formalisms and main techniques used to construct them, make predictions, and support decision-making under uncertainty.

The aim of this course is to develop the knowledge and skills necessary to design, implement and apply these models to solve real problems. The course will cover: 
- (1) Bayesian networks, undirected graphical models and their temporal extensions
- (2) exact and approximate inference methods
- (3) estimation of the parameters and the structure of graphical models.

BOOK : [Probabilistic Graphical Models: Principles and Techniques by Daphne Koller and Nir Friedman.](https://github.com/SKKSaikia/CS228_PGM/blob/master/Probabilistic%20Graphical%20Models%20-%20Principles%20and%20Techniques.pdf)

ð“„† <b>Important Books : </b><br/>
ð“Š– [Modeling and Reasoning with Bayesian](https://github.com/SKKSaikia/CS228_PGM/blob/master/Modeling%20and%20Reasoning%20with%20Bayesian.pdf) <br/>
ð“Š– [Information Theory, Inference, and Learning Algorithms](https://github.com/SKKSaikia/CS228_PGM/blob/master/books/Information%20Theory%2C%20Inference%2C%20and%20Learning%20Algorithms%20by%20David%20J.%20C.%20Mackay.pdf) <br/>
ð“Š– [Machine Learning A Probabilistic Perspective](https://doc.lagout.org/science/Artificial%20Intelligence/Machine%20learning/Machine%20Learning_%20A%20Probabilistic%20Perspective%20%5BMurphy%202012-08-24%5D.pdf) <br/>
ð“Š– [Bayesian Reasoning and Machine Learning by David Barber](https://github.com/SKKSaikia/CS228_PGM/blob/master/books/Bayesian%20Reasoning%20and%20Machine%20Learning%20by%20David%20Barber.pdf) <br/>
ð“Š– [Graphical models, exponential families, and variational inference](https://github.com/SKKSaikia/CS228_PGM/blob/master/Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%20by%20Martin%20J.%20Wainwright%20and%20Michael%20I.%20Jordan.pdf) <br/>

<b> Homework (70%) + Final Exam (30%) </b>| Homework - Theoretical + Programming

# Homework

# [COURSE](https://ermongroup.github.io/cs228-notes/)

<h2><b> â™ž PRELIMINARIES </b></h2>

- Introduction: What is probabilistic graphical modeling? Overview of the course. <br/>
- Review of probability theory: Probability distributions. Conditional probability. Random variables (under construction). <br/>
- Examples of real-world applications: Image denoising. RNA structure prediction. Syntactic analysis of sentences. Optical character recognition (under construction). <br/>

<h2><b> â™ž REPRESENTATION </b></h2>

- Bayesian networks: Definitions. Representations via directed graphs. Independencies in directed models. <br/>
- Markov random fields: Undirected vs directed models. Independencies in undirected models. Conditional random fields. <br/>

<h2><b> â™ž INFERENCE </b></h2>

- Variable elimination The inference problem. Variable elimination. Complexity of inference. <br/>
- Belief propagation: The junction tree algorithm. Exact inference in arbitrary graphs. Loopy Belief Propagation. <br/>
- MAP inference: Max-sum message passing. Graphcuts. Linear programming relaxations. Dual decomposition. <br/>
- Sampling-based inference: Monte-Carlo sampling. Importance sampling. Markov Chain Monte-Carlo. Applications in inference. <br/>
- Variational inference: Variational lower bounds. Mean Field. Marginal polytope and its relaxations. <br/>

<h2><b> â™ž LEARNING </b></h2>

- Learning in directed models: Maximum likelihood estimation. Learning theory basics. Maximum likelihood estimators for Bayesian networks. <br/>
- Learning in undirected models: Exponential families. Maximum likelihood estimation with gradient descent. Learning in CRFs <br/>
- Learning in latent variable models: Latent variable models. Gaussian mixture models. Expectation maximization. <br/>
- Bayesian learning: Bayesian paradigm. Conjugate priors. Examples (under construction). <br/>
- Structure learning: Chow-Liu algorithm. Akaike information criterion. Bayesian information criterion. Bayesian structure learning (under construction). <br/>

<h2><b> â™ž BRINGING IT ALL TOGETHER </b></h2>

- The variational autoencoder: Deep generative models. The reparametrization trick. Learning latent visual representations. <br/>
- List of further readings: Structured support vector machines. Bayesian non-parametrics. <br/>

# FINAL EXAM

